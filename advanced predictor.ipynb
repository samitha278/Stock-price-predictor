{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b413451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tvDatafeed import TvDatafeed, Interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3536fe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tvDatafeed.main:you are using nologin method, data you access may be limited\n"
     ]
    }
   ],
   "source": [
    "tv = TvDatafeed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b61cae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "comb = tv.get_hist(symbol='COMB.N0000',exchange='CSELK',interval=Interval.in_daily,n_bars=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 206ms/step - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 162ms/step - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 190ms/step - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m51/75\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 116ms/step - loss: nan"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "\n",
    "# Example NumPy array (replace this with your actual data)\n",
    "data = np.array(comb)  # data.shape - (3030, 6)\n",
    "\n",
    "# Convert the NumPy array to a DataFrame\n",
    "columns = [\"symbol\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Convert relevant columns to numeric\n",
    "df[\"open\"] = df[\"open\"].astype(float)\n",
    "df[\"high\"] = df[\"high\"].astype(float)\n",
    "df[\"low\"] = df[\"low\"].astype(float)\n",
    "df[\"close\"] = df[\"close\"].astype(float)\n",
    "df[\"volume\"] = df[\"volume\"].astype(float)\n",
    "\n",
    "# Ensure the DataFrame index is a DatetimeIndex\n",
    "df['date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='D')\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Add technical indicators\n",
    "df = dropna(df)  # Drop NaN values\n",
    "df = add_all_ta_features(\n",
    "    df, open=\"open\", high=\"high\", low=\"low\", close=\"close\", volume=\"volume\", fillna=True\n",
    ")\n",
    "\n",
    "# Identify support and resistance levels\n",
    "def identify_support_resistance(df, window=5):\n",
    "    df['support'] = df['low'].rolling(window=window, center=True).min()\n",
    "    df['resistance'] = df['high'].rolling(window=window, center=True).max()\n",
    "    return df\n",
    "\n",
    "df = identify_support_resistance(df)\n",
    "\n",
    "# Define features and target\n",
    "features = [\n",
    "    \"open\", \"high\", \"low\", \"close\", \"volume\",  # Original features\n",
    "    \"trend_macd\", \"trend_macd_signal\", \"trend_macd_diff\",  # MACD\n",
    "    \"momentum_rsi\",  # RSI\n",
    "    \"volatility_bbm\", \"volatility_bbh\", \"volatility_bbl\",  # Bollinger Bands\n",
    "    \"trend_ema_fast\", \"trend_ema_slow\",  # Exponential Moving Averages\n",
    "    \"support\", \"resistance\"  # Support and Resistance\n",
    "]\n",
    "target = \"close\"\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(df, features, target, time_steps=60):\n",
    "    if len(df) < time_steps + 1:\n",
    "        raise ValueError(f\"Not enough data to create sequences. Required: {time_steps + 1}, Available: {len(df)}\")\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(df[features])\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(time_steps, len(scaled_data)):\n",
    "        X.append(scaled_data[i-time_steps:i, :])\n",
    "        y.append(scaled_data[i, features.index(target)])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "# Define time steps and preprocess data\n",
    "time_steps = 60  # Use 60 time steps for better context\n",
    "try:\n",
    "    X, y, scaler = preprocess_data(df, features, target, time_steps)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "    exit()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Build the LSTM model\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        LSTM(150, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(150, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(100),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    return model\n",
    "\n",
    "# Create and train the model\n",
    "model = build_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()\n",
    "\n",
    "# Function to predict future prices (updated with debug statements)\n",
    "def predict_future_prices(model, last_sequence, scaler, time_steps, future_days):\n",
    "    if last_sequence.shape[0] < time_steps:\n",
    "        raise ValueError(f\"last_sequence must have at least {time_steps} time steps. Found: {last_sequence.shape[0]}\")\n",
    "    \n",
    "    predictions = []\n",
    "    current_sequence = last_sequence[-time_steps:].copy()\n",
    "    \n",
    "    for _ in range(future_days):\n",
    "        next_prediction = model.predict(current_sequence[np.newaxis, :, :])[0, 0]\n",
    "        predictions.append(next_prediction)\n",
    "        \n",
    "        # Update the sequence with the new prediction\n",
    "        new_row = np.append(current_sequence[-1, 1:], next_prediction).reshape(1, -1)\n",
    "        current_sequence = np.vstack([current_sequence, new_row])\n",
    "        current_sequence = current_sequence[1:]  # Remove the oldest time step\n",
    "    \n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    close_scaler = MinMaxScaler()\n",
    "    close_scaler.min_, close_scaler.scale_ = scaler.min_[features.index(target)], scaler.scale_[features.index(target)]\n",
    "    predictions = close_scaler.inverse_transform(predictions)\n",
    "    return predictions\n",
    "\n",
    "# Predict prices for the next month (30 days)\n",
    "try:\n",
    "    last_sequence = X_test[-1]  # Use the last sequence from the test data\n",
    "    future_days = 30  # Predict for the next 30 days\n",
    "    predictions = predict_future_prices(model, last_sequence, scaler, time_steps, future_days)\n",
    "except ValueError as e:\n",
    "    print(f\"Error in predict_future_prices: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Convert predictions into DataFrame\n",
    "dates = pd.date_range(start=df.index[-1], periods=future_days + 1, freq='D')[1:]\n",
    "pred_df = pd.DataFrame(predictions, columns=[\"close\"], index=dates)\n",
    "pred_df[\"open\"] = pred_df[\"close\"].shift(1)\n",
    "pred_df[\"open\"].fillna(pred_df[\"close\"].iloc[0], inplace=True)\n",
    "pred_df[\"high\"] = pred_df[[\"open\", \"close\"]].max(axis=1)\n",
    "pred_df[\"low\"] = pred_df[[\"open\", \"close\"]].min(axis=1)\n",
    "pred_df[\"volume\"] = 0  # No volume for predictions\n",
    "\n",
    "# Plot original data as candlestick\n",
    "mpf.plot(df[-100:], type='candle', style='charles', title=\"Stock Price History\", ylabel=\"Price\")\n",
    "\n",
    "# Plot future predictions as candlestick\n",
    "if not pred_df.empty:\n",
    "    mpf.plot(pred_df.dropna(), type='candle', style='charles', title=\"Next Month Price Prediction\", ylabel=\"Price\")\n",
    "else:\n",
    "    print(\"No predictions to plot. Check the predict_future_prices function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fe0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
